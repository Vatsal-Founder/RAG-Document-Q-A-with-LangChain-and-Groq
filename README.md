# RAG Document Q\&A â€” LangChain + Groq + FAISS

Endâ€‘toâ€‘end **RAG** application that answers questions over your **internal PDFs** using:

* **Groq** for fast, lowâ€‘latency LLM inference (chat generation)
* **OpenAI embeddings** for vectorization
* **FAISS** for vector search
* **LangSmith** for tracing and run analytics

> Repo: `Vatsal-Founder/RAG-Document-Q-A-with-LangChain-and-Groq`


Link: 

---

## Features

* ðŸ“„ **Bring your own PDFs** â€” drop files into a folder and index them.
* ðŸ§­ **Accurate retrieval** with FAISS and chunking.
* âš¡ **Groq LLM** (e.g., Llamaâ€‘3 family) for fast responses.
* ðŸ”Ž **OpenAI embeddings** (swappable) for highâ€‘quality retrieval.
* ðŸ§© **LangChain** orchestration with optional LangSmith traces.

---

## Architecture

```
[User] â†’ [Chat UI / CLI]
            â†“
       [LangChain App]
   â”œâ”€ Retriever (FAISS over PDF chunks)
   â”œâ”€ Embeddings (OpenAI)
   â””â”€ Generator (Groq LLM)
            â†“
        [Answer + Sources]
```

---

## Project Structure (typical)

```
.
â”œâ”€â”€ app.py                 # entrypoint (chat UI or server)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ data/
â”‚   â””â”€â”€ pdfs/              # place your PDFs here
â””â”€â”€ README.md
```

---

## Requirements

* Python 3.10+
* OpenAI API key (embeddings)
* Groq API key (LLM)

---

## Setup

```bash
git clone https://github.com/Vatsal-Founder/RAG-Document-Q-A-with-LangChain-and-Groq.git
cd RAG-Document-Q-A-with-LangChain-and-Groq
python -m venv .venv && source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt

# create .env with your secrets
cp .env.example .env  # if the file exists; otherwise create it
```

### Environment variables (minimum)

```ini
# LLM (Groq)
GROQ_API_KEY=your_groq_key
GROQ_MODEL=llama-3.1-8b  # example; set to your preferred Groq model

# Embeddings (OpenAI)
OPENAI_API_KEY=your_openai_key
EMBEDDINGS_MODEL=text-embedding-3-small


# LangSmith (optional)
LANGCHAIN_TRACING_V2=true
LANGCHAIN_API_KEY=your_langsmith_key
LANGCHAIN_PROJECT=rag-docs
```

---


## Run locally

Choose the mode that matches `app.py` (UI or server). If unsure, try Streamlit first.

### Streamlit UI

```bash
streamlit run app.py
```

Open [http://localhost:8501](http://localhost:8501) and start asking questions about your PDFs.


---



## How to Use (at a glance)

1. **Ingest** your PDFs.
2. **Run** the app (Streamlit or server).
3. **Ask** natural language questions.
4. See **answers + cited snippets/sources** (if implemented in your UI).

---

## Configuration Tips

* Start with `text-embedding-3-small` for speed/cost; upgrade to `text-embedding-3-large` for best recall.
* Tune `chunk-size`/`overlap` for your PDFs (technical docs often benefit from \~1000/150).
* Keep your FAISS index folder outside of transient build directories so it survives container rebuilds.

---

